[
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/input/",
	"title": "Data Capture input",
	"tags": [],
	"description": "",
	"content": "The input of Data Capture is TODO.\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/input/",
	"title": "Data Factory input",
	"tags": [],
	"description": "",
	"content": "The input of Data Factory is the output of Data Capture.\nFrom Data Capture output specifications Depersonalised DICOM + EHR data export  ├── DICOM │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.dcm -- set of DICOM files │ └── dicom_name_generated_02.dcm -- set of DICOM files │ └── dicom_name_generated_03.dcm -- set of DICOM files └── EHR └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  Depersonalised Nifti + EHR data export  ├── NIFTI │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.nifti -- Nifti file │ └── dicom_name_generated_01.json -- metadata for the Nifti file │ └── dicom_name_generated_02.nifti -- Nifti file │ └── dicom_name_generated_02.json -- metadata for the Nifti file └── EHR └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  scan_research_id: an ID for research, with no identifier coming the clinical database and representing one visit for one patient. During this visit, there may be more than one scan acquisition session, each session can have several sequences, a sequence can have several repetitions and acquire as many brain scan. One brain scan can be spread into several DICOM files where each file represents a slice of the brain.\n{{% alert theme=\u0026ldquo;warning\u0026rdquo; %}} After de-identification, we should ensure that patient IDs present in the EHR data match patient IDs present in the DICOM headers. {{% /alert %}}   "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/output/",
	"title": "Data Capture output",
	"tags": [],
	"description": "",
	"content": "The Data Capture should export its data into a folder made available to the Data Factory\nThere are several options possible, to adapt to the local requirements:\nDepersonalised DICOM + EHR data export  ├── DICOM │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.dcm -- set of DICOM files │ └── dicom_name_generated_02.dcm -- set of DICOM files │ └── dicom_name_generated_03.dcm -- set of DICOM files └── EHR └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  Depersonalised Nifti + EHR data export  ├── NIFTI │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.nifti -- Nifti file │ └── dicom_name_generated_01.json -- metadata for the Nifti file │ └── dicom_name_generated_02.nifti -- Nifti file │ └── dicom_name_generated_02.json -- metadata for the Nifti file └── EHR └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  scan_research_id: an ID for research, with no identifier coming the clinical database and representing one visit for one patient. During this visit, there may be more than one scan acquisition session, each session can have several sequences, a sequence can have several repetitions and acquire as many brain scan. One brain scan can be spread into several DICOM files where each file represents a slice of the brain.\nAfter de-identification, we should ensure that patient IDs present in the EHR data match patient IDs present in the DICOM headers.\n "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/output/",
	"title": "Data Factory output",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": " Medical Informatics Platform\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/hospital-database/",
	"title": "Hospital Database",
	"tags": [],
	"description": "",
	"content": " Chapter X Some Chapter title Lorem ipsum\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/algorithm-factory/",
	"title": "Algorithm Factory",
	"tags": [],
	"description": "",
	"content": " Chapter X Some Chapter title Lorem ipsum\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/",
	"title": "Data Capture",
	"tags": [],
	"description": "",
	"content": " Goals The main role of the Data Capture is to take data coming several clinical systems in a hospital, collect that data and remove any identifying information from them, then export that data as a collection of CSV files and DICOM/Nifti files to be processed by the Data Factory.\n disparate clinical data =\u0026gt; collection + de-identification =\u0026gt; export to exchange directory  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/",
	"title": "Data Factory",
	"tags": [],
	"description": "",
	"content": " Goals The main role of the Data Factory is to take data coming from Data capture , process it offline (without any user interaction) then store the results into a database of the Hospital Database Bundle.\n anonymised data =\u0026gt; pre-processing + ETL + automated curation =\u0026gt; data store  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/",
	"title": "MIP specs",
	"tags": [],
	"description": "",
	"content": " Specification for the Medical Informatics Platform Flow of data in the Platform graph BT subgraph Clinical data or research cohorts in_ehr(EHR database) in_pacs(PACS) end subgraph Data Capture subgraph Anonymiser dc_anon_ehrDepersonalisation of EHR data] dc_anon_mriDepersonalisation of MRI scans] end end subgraph Data Factory subgraph Workflow engine airflow_mri_preprocessingMRI pre-processing] -- airflow_feature_extractionFeature Extraction] airflow_ehr_versionVersioning] -- airflow_ehr_harmoniseHarmonisation] end df_i2b2(I2B2 database) airflow_feature_extraction -- df_i2b2 airflow_ehr_harmonise -- df_i2b2 end subgraph Algorithm Factory hd_features(Features database) --- af_worker(Algorithms) end in_ehr --|Patients with consent| dc_anon_ehr in_pacs --|Patients with consent| dc_anon_mri dc_anon_ehr -- airflow_ehr_version dc_anon_mri -- airflow_mri_preprocessing df_i2b2 -- hd_features  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/algorithm-factory/",
	"title": "Algorithm-factories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/hospital-database/",
	"title": "Hospital-databases",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]