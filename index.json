[
{
	"uri": "https://hbpmedical.github.io/specifications/data-flow/",
	"title": "Data flow",
	"tags": [],
	"description": "Flow of data in the platform",
	"content": "One of the main purpose of the Medical Informatics Platform is to collect and process clinical and research data and put into the hand of its users (clinicians, researchers in neuroscience) ready-to-use data containing the variables and features made available for research.\nThe machine learning algorithms and statistical analysis methods provided by the platform are using this feature data when a user is exploring data or performing an experiment.\n Flow in MIP Local  \n Flow in MIP Federation  \n "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-flow/mip-local/",
	"title": "Flow in MIP Local",
	"tags": [],
	"description": "",
	"content": "MIP Local is installed in participating hospitals and used to collect clinical data provided by the hospital. Only patients with consent are selected, and no data ever leaves the hospital premises.\n\nFlow of data in MIP Local installed at a hospital graph BT subgraph Clinical data or research cohorts in_ehr(EHR database) in_pacs(PACS) end subgraph Data Capture subgraph Anonymiser dc_anon_ehrDepersonalisation of EHR data] dc_anon_mriDepersonalisation of MRI scans] end end subgraph Data Factory subgraph Workflow engine airflow_mri_preprocessingMRI pre-processing] -- airflow_feature_extractionFeature Extraction] airflow_ehr_versionVersioning] -- airflow_ehr_harmoniseHarmonisation] end df_i2b2(I2B2 database) airflow_feature_extraction -- df_i2b2 airflow_ehr_harmonise -- df_i2b2 end subgraph Algorithm Factory hd_features(Features database) --- af_worker(Algorithms) end in_ehr --|Patients with consent| dc_anon_ehr in_pacs --|Patients with consent| dc_anon_mri dc_anon_ehr -- airflow_ehr_version dc_anon_mri -- airflow_mri_preprocessing df_i2b2 -- hd_features "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-flow/mip-federated/",
	"title": "Flow in MIP Federation",
	"tags": [],
	"description": "",
	"content": "MIP Federated brings together several hospitals and clinical data centers to build a Federation.\nData are collected at each hospital (clinical data) or clinical data centers (research data), then a user of MIP can query and do machine learning or statistical analyses but with the strong imperative that only data aggregates are exchanged between the hospital or data centers and the central node hosting the user-facing web site.\n\nFlow of data in MIP Federation graph BT subgraph Hospital #1 databases in_data(EHR database, PACS) end subgraph MIP Local at Hospital #1 subgraph Data Capture dc_anonDepersonalisation] end subgraph Data Factory df_preprocessingPre-processing, feature extraction] end subgraph Algorithm Factory and Local database hd_af(Features database and algorithms) end end in_data --|Patients with consent| dc_anon dc_anon -- df_preprocessing df_preprocessing --hd_af subgraph Hospital #2 databases in2_data(EHR database, PACS) end subgraph MIP Local at Hospital #2 subgraph Data Capture dc2_anonDepersonalisation] end subgraph Data Factory df2_preprocessingPre-processing, feature extraction] end subgraph Algorithm Factory and Local database hd2_af(Features database and algorithms) end end in2_data --|Patients with consent| dc2_anon dc2_anon -- df2_preprocessing df2_preprocessing -- hd2_af subgraph Central node inc_data(Research cohorts) end subgraph MIP Local on central node subgraph Data Capture dcc_anonDepersonalisation] end subgraph Data Factory dfc_preprocessingPre-processing, feature extraction] end subgraph Algorithm Factory and Local database hdc_af(Features database and algorithms) end end inc_data --|Subjects with consent| dcc_anon dcc_anon -- dfc_preprocessing dfc_preprocessing -- hdc_af subgraph Federation layer web(Web analysis) dist_soft(Distributed database and algorithms) web --- dist_soft end hd_af --|Aggregates|dist_soft hd2_af --|Aggregates|dist_soft hdc_af --|Aggregates|dist_soft user((User)) --- web "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/",
	"title": "Data Capture",
	"tags": [],
	"description": "Goals of Data Capture",
	"content": "The main role of the Data Capture is to take data coming several clinical systems in a hospital, collect that data and remove any identifying information from them, then export that data as a collection of CSV files and DICOM/Nifti files to be processed by the Data Factory.\ngraph LR data_in(Disparate clinical data) data_out(Export to data exchange directory) processing Collection + curation + de-identification] data_in -- processing processing -- data_out  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/input/",
	"title": "Data Capture input",
	"tags": [],
	"description": "",
	"content": " Data Capture collects available data from hospital databases or research cohorts.\nIn addition, clinical data are de-personalised to protect patient privacy.\nIt should also track consent and trigger the appropriate mechanisms to remove data from the platform in case the patient has removed her consent.\nIdeal forms of data repository for MIP The MIP has been designed to support a very wide range of methods to gather data, however some datasets are easier to include in the MIP if they meet some criteria.\nDe-personalised data It can be seen as safer and more secure for a hospital to take care of the de-personalisation of its data, especially as this procedure may expose the hospital to ethical and legal questions.\nWe encourage hospitals to provide de-personalised data to MIP, and we can also help to put in place a technical solution, FedEHR\u0026copy; Anonymizer\u0026copy;, provided by our partner Gnubila.\nI2B2 compatible databases MIP uses internally in its Data Factory several databases based on the I2B2 schema. If your data has been prepared for data sharing and follows the I2B2 standard, this effort will fasten the inclusion of your data in the platform.\nPACS system MIP can connect to and query any PACS system compatible with the DICOM standard.\nTo better protect your clinical PACS, we recommend that you put in place a research PACS that will contain only the selection of medical images available for research.\nOther ways of integrating data in the MIP Provided some development or configuration on our side, we can support the following methods of integrating data into MIP.\nFiles extract We can ingest data extract provided as a set of files.\nFor MRI data, files can be in the DICOM or NIFTI formats. We support in particular the BIDS organisation of data\nFor EHR data, files can be in CVS, TSV, JSON or XML formats.\nWeb services We can customise the data capture to use online research databases with a Web Service API.\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/anonymizer/",
	"title": "Anonymisation module",
	"tags": [],
	"description": "",
	"content": "A module that performs anonymisation is provided by MIP when the hospital does not have the tools to perform the de-personalisation of all data.\nThe following is used only when the GNUBILA Pandora FedEHR anonymiser has been setup.\n Data folder organisation for the anonymisation processing The software GNUBILA Pandora FedEHR is used to perform the de-personalisation of all EHR and imaging data.\n /data ├── anonymiser -- This folder contains all the data being processed. │ ├── db -- This contains the database of the internal IDs to public IDs mappings. │ ├── in -- Files to be processed. │ ├── out -- Files successfuly anonymised. │ ├── quarantine -- Files for which the anonymisation failed: │ │ ├── csv -- * EHR data files │ │ ├── dicom -- * Imaging data files │ │ ├── pacs_csv -- * EHR data retrieved through a PACS connexion │ │ └── unknown -- * Any other kind of failures │ └── scripts -- Anonymisation configurations for: │ ├── csv -- * EHR data files │ └── dicom -- * Imaging data files └── ldsm  Please be aware that as part of the anonymisation, files are moved from the input folder either to the quarantine folder, or to the output folder (and modified).\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/output/",
	"title": "Data Capture output",
	"tags": [],
	"description": "",
	"content": "The Data Capture should export its data into a folder made available to the Data Factory\nThere are several options possible, to adapt to the local requirements:\nDepersonalised DICOM + EHR data export  ├── DICOM │ └── 2016 -- yearly folder, date represents the date of export │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.dcm -- set of DICOM files │ └── dicom_name_generated_02.dcm -- set of DICOM files │ └── dicom_name_generated_03.dcm -- set of DICOM files └── EHR └── 2016 -- yearly folder, date represents the date of export └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  Depersonalised Nifti + EHR data export  ├── NIFTI │ └── 2016 -- yearly folder, date represents the date of export │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.nifti -- Nifti file │ └── dicom_name_generated_01.json -- metadata for the Nifti file │ └── dicom_name_generated_02.nifti -- Nifti file │ └── dicom_name_generated_02.json -- metadata for the Nifti file └── EHR └── 2016 -- yearly folder, date represents the date of export └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  scan_research_id: an ID for research, with no identifier coming the clinical database and representing one visit for one patient. During this visit, there may be more than one scan acquisition session, each session can have several sequences, a sequence can have several repetitions and acquire as many brain scan. One brain scan can be spread into several DICOM files where each file represents a slice of the brain.\nAfter de-identification, we should ensure that patient IDs present in the EHR data match patient IDs present in the DICOM headers.\n "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/",
	"title": "Data Factory",
	"tags": [],
	"description": "Goals of Data Factory",
	"content": " Goals The main role of the Data Factory is to take data coming from Data capture , process it offline (without any user interaction) then store the results into a database of the Hospital Database Bundle.\ngraph LR data_in(Anonymised data) data_out(Data store) processing Pre-processing + ETL + automated curation] data_in -- processing processing -- data_out  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/input/",
	"title": "Data Factory input",
	"tags": [],
	"description": "",
	"content": "The input of Data Factory is the output of Data Capture.\nFrom Data Capture output specifications Depersonalised DICOM + EHR data export  ├── DICOM │ └── 2016 -- yearly folder, date represents the date of export │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.dcm -- set of DICOM files │ └── dicom_name_generated_02.dcm -- set of DICOM files │ └── dicom_name_generated_03.dcm -- set of DICOM files └── EHR └── 2016 -- yearly folder, date represents the date of export └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  Depersonalised Nifti + EHR data export  ├── NIFTI │ └── 2016 -- yearly folder, date represents the date of export │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.nifti -- Nifti file │ └── dicom_name_generated_01.json -- metadata for the Nifti file │ └── dicom_name_generated_02.nifti -- Nifti file │ └── dicom_name_generated_02.json -- metadata for the Nifti file └── EHR └── 2016 -- yearly folder, date represents the date of export └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data    In addition, currently our MRI image pre-processing pipelines are optimised to work only with well-defined medical images.\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/images/",
	"title": "Requirements for imaging data",
	"tags": [],
	"description": "",
	"content": " In order to be pre-processed by the Data Factory, the imaging data have to meet some requirements, both regarding the images format and the images meta-data.\nImages format  The images must be full brain scans. The images must be provided either in DICOM or NIFTI format. The images must be high-resolution (max. 1.5 mm) T1-weighted sagittal images. If the dataset contains other types of images (that is not meeting the above description, e.g. fMRI data, T2 images, etc), a list of protocol names used and their compatibility status regarding the above criterion has to be provided. The images must contain at least 40 slices.  Images Meta-data Imaging meta-data typically are the information contained in the DICOM tags. If the images are provided in NIFTI format or some of the mandatory DICOM tags are missing from the images, those meta-data must be provided in one or several extra file(s) (e.g. JSON, CSV, etc).\nHere is a list of DICOM tags we are interested in :\n   TAG TYPE MANDATORY DESCRIPTION     PatientID LO Yes Patient identifier.   StudyID SH Yes Study identifier. Used to identify a visit. (Unique per dataset or per patient)   SeriesDescription LO Yes Series description. Used to describe the scanning sequence/protocol. Must be stable over time.   SeriesNumber IS Yes Series number. Used to identify a scan run.   InstanceNumber IS Yes Image (slice) identifier.   ImagePosition DS Yes Image (slice) position.   ImageOrientation DS Yes Image (slice) orientation.   SliceLocation DS Yes Slice location.   SamplesPerPixel US Yes Number of samples (planes) per pixel. Usually, 1 for monochrome (gray scale) and palette color images, or 3 for RGB images.   Rows US Yes Number of rows in the image.   Columns US Yes Number of columns in the image.   PixelSpacing DS Yes Distance between the center of each pixel.   BitsAllocated US Yes Number of bits allocated for each pixel sample.   BitsStored US Yes Number of bits stored for each pixel sample.   HighBit US Yes Most significant bit for pixel sample data.   AcquisitionDate DA 1 Acquisition date. We try to use it as the scan date.   SeriesDate DA 1 Series (scan run) date. If AcquisitionDate is missing, we use it as a scan date.   PatientAge AS 2 Patient age at scan date.   PatientBirthDate DA 2 Patient\u0026rsquo;s birth date.   MagneticFieldStrength DS No Magnetic field strength.   PatientSex LO No Patient gender.   Manufacturer LO No Scanner manufacturer.   ManufacturerModelName LO No Scanner model name.   InstitutionName LO No Institution name.   StudyDescription LO No Study (visit) description.   SliceThickness DS No Slice thickness in mm.   RepetitionTime DS No The period of time in msec between the beginning of a pulse sequence and the beginning of the succeeding (essentially identical) pulse sequence.   EchoTime DS No Time in ms between the middle of the excitation pulse and the peak of the echo produced (kx=0).   SpacingBetweenSlices DS No Spacing between slices in mm (from center to center).   NumberOfPhaseEncodingSteps IS No Total number of lines in k-space in the \u0026lsquo;y\u0026rsquo; direction collected during acquisition.   EchoTrainLength IS No Number of lines in k-space acquired per excitation per image.   PercentPhaseFieldOfView DS No Ratio of field of view dimension in phase direction to field of view dimension in frequency direction, expressed as a percent.   PixelBandwidth DS No Reciprocal of the total sampling period, in hertz per pixel.   FlipAngle DS No Steady state angle in degrees to which the magnetic vector is flipped from the magnetic vector of the primary field.   PercentSampling DS No Fraction of acquisition matrix lines acquired, expressed as a percent.   EchoNumber IS No The echo number used in generating this image. In the case of segmented k-space, it is the effective Echo Number.    For more information about the attributes type, have a look at : http://northstar-www.dartmouth.edu/doc/idl/html_6.2/Value_Representations.html.\nNOTE: If the \u0026lsquo;MANDATORY\u0026rsquo; column contains a number, only one of the tags marked with the same number is needed (e.g. only one of the \u0026lsquo;PatientAge\u0026rsquo; and \u0026lsquo;PatientBirthDate\u0026rsquo; tags is needed).\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/pipelines/",
	"title": "Processing pipelines",
	"tags": [],
	"description": "",
	"content": " The processing pipelines provided out-of-the-box by the Data Factory enable an automated processing of data made available to MIP Local or MIP Federated.\nOverview of all pipelines graph LR data_in(Anonymised data from Data Capture or other sources) data_out(Research-grade data) reorg_pipeline Reorganisation pipeline] ehr_pipeline EHR curation pipeline] metadata_pipeline Metadata curation pipeline] preprocessing_pipeline MRI pre-processing and feature extraction pipeline] normalisation_pipeline Normalisation and data export pipeline] data_in -- reorg_pipeline reorg_pipeline -- ehr_pipeline reorg_pipeline -- metadata_pipeline reorg_pipeline -- preprocessing_pipeline ehr_pipeline -- normalisation_pipeline metadata_pipeline -- normalisation_pipeline preprocessing_pipeline -- normalisation_pipeline normalisation_pipeline -- data_out  Reorganisation pipeline This pipeline takes data organised on the disk in its original format and reorganise it into something that fits the layout expected by the following pipelines (EHR, pre-processing, metadata).\ngraph LR data_in(Anonymised data from Data Capture or other sources) data_out(Reorganised data) processing Reorganisation of MRI scans and EHR data] data_in -- processing processing -- data_out  EHR curation pipeline This pipeline captures as many variables as possible from the patient records and stores the data into a database compliant with I2B2 schema (\u0026lsquo;I2B2 capture\u0026rsquo; database)\ngraph LR data_in(CSV files or other files containing EHR data) data_out(I2B2 capture database) processing ETL with light mapping of EHR data to I2B2 schema] data_in -- processing processing -- data_out  Metadata curation pipeline This pipeline collects the information associated with MRI scans and present either in DICOM headers or in associated metadata files and stores in into the (\u0026lsquo;I2B2 capture\u0026rsquo; database)\ngraph LR data_in(Metadata extracted from MRI scans) data_out(I2B2 capture database) processing ETL with light mapping of metadata to I2B2 schema] data_in -- processing processing -- data_out  MRI pre-processing and feature extraction pipeline This pipeline takes MRI data organised following the directory structure /PatientID/StudyID/SeriesProtocol/SeriesID/ and applies a series of processing steps on it, including:\n Conversion from DICOM to Nifti Neuromorphometric pipeline Quality control  For each step, data provenance is tracked and stored in a \u0026lsquo;Data Catalog\u0026rsquo; database.\ngraph LR data_in(MRI scans) data_out(Features stored into 'I2B2 capture' database) processing Neuromorphometric pipeline + quality control + provenance] data_in -- processing processing -- data_out  Normalisation and data export pipeline This pipeline is triggered on a patient record when there is enough information collected (both EHR and biomarkers from MRI are required). It uses the data mapping and transformation specifications provided by the DGDS committee to select the variables of interest and normalise them into the MIP Common Data Elements reference.\ngraph LR data_in('I2B2 capture' database) data_normalised('I2B2 MIP CDE' database) data_out(Features table containing research-grade data) processing Selection of variables and normalisation] export Export MIP CDE variables and other variables to a Features table] data_in -- processing processing -- data_normalised data_normalised -- export export -- data_out  This pipeline provides the final results produced by the Data Factory.\nFrom Data Factory output specifications The ouput of Data Factory is a set of research-grade data containing the biomarkers extracted from MRI scans and the variables extracted from the patient (or research subject) EHR records.\nThis information is sent to the Hospital Database and provided to the machine learning and statistical analysis algorithms of the Algorithm Factory as well as the distributed queries when the instance of MIP at a hospital is connected to the Federation.   "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/capture_i2b2/",
	"title": "Capture Database",
	"tags": [],
	"description": "Database with an I2B2 schema for storing all information harvested onsite",
	"content": " Database with an I2B2 schema storing all information harvested onsite The Capture database enables the collection of lots of information coming from the hospital EHR data or from research cohort metadata in an unaltered way.\nAt this point, no attempt is made to select or transform the data to fit into the MIP Common Data Elements.\nThis database serves purely the data curation process: data elements are identified, described, collected and managed in a database.\nTo support the capture of as much information as possible, we have adopted the widely used I2B2 database schema, a variant of the star schema commonly used in data marts\nAn interesting consequence of this choice is that as several research databases and hospital data exports for research use I2B2, the data importation process is a simple database copy.\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/cde/",
	"title": "Common Data Elements",
	"tags": [],
	"description": "Definition of variables for MIP",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/cde_i2b2/",
	"title": "CDE-normalised database",
	"tags": [],
	"description": "Normalise captured data and store it into an I2B2 database",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/provenance/",
	"title": "Provenance",
	"tags": [],
	"description": "Tracking provenance information in the Data catalog",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/output/",
	"title": "Data Factory output",
	"tags": [],
	"description": "",
	"content": " The ouput of Data Factory is a set of research-grade data containing the biomarkers extracted from MRI scans and the variables extracted from the patient (or research subject) EHR records.\nThis information is sent to the Hospital Database and provided to the machine learning and statistical analysis algorithms of the Algorithm Factory as well as the distributed queries when the instance of MIP at a hospital is connected to the Federation.\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/algorithm-factory/",
	"title": "Algorithm Factory",
	"tags": [],
	"description": "",
	"content": " Chapter X Some Chapter title Lorem ipsum\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/hospital-database/",
	"title": "Hospital Database",
	"tags": [],
	"description": "",
	"content": " Import of data in LDSM Once the harmonised data is available in the i2b2, it is made available in the LDSM through csv files and exposed to the Web Portal and Exareme as tables.\nThe current schema is based on the requirement of the Web Portal and the available variables. Currently (July 2017), the schema used is a big table containing one column per variable, and one line per patient and visit. The schema will be further refined in the coming months based on available data and MIP requested features.\nMIPMap is used to extract all the relevant data from the i2b2 harmonised database and create csv files with the expected schema. The current table is named harmonised_clinical_data.csv. Currently, this file is directly saved in the \u0026ldquo;datasets\u0026rdquo; folder of PostgresRAW-UI. Once the LDSM is moved to a different server, this will be replaced by a scp command to copy the file on the distant server. Authentification will be key-pair-based.\nAll files in the LDSM dataset folder are automatically detected by PostgresRAW-UI, which infers their schema and registers the files as tables in PostgresRAW. The data can then be queried in PostgresRAW in the table harmonised_clinical_data.\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/hospital-database/distributed-queries/",
	"title": "Distributed queries",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/credits/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": " Github contributors .ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start } .ghContributors  div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors  div label{ padding-left: 4px ; } .ghContributors  div span{ font-size: x-small; padding-left: 4px ; }   @ludovicc 39 commits   @sambuc 1 commits   @dianeperez 1 commits   Tooling  Netlify - Continuous deployment and hosting of this documentation Hugo   hugo-theme-docdock    "
},
{
	"uri": "https://hbpmedical.github.io/specifications/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": " Medical Informatics Platform\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/",
	"title": "MIP specs",
	"tags": [],
	"description": "",
	"content": " Specification for the Medical Informatics Platform  Data flow  Flow of data in the platform\n Data Capture  Goals of Data Capture\n Data Factory  Goals of Data Factory\n Algorithm Factory  Chapter X Some Chapter title Lorem ipsum\n Hospital Database  Import of data in LDSM Once the harmonised data is available in the i2b2, it is made available in the LDSM through csv files and exposed to the Web Portal and Exareme as tables. The current schema is based on the requirement of the Web Portal and the available variables. Currently (July 2017), the schema used is a big table containing one column per variable, and one line per patient and visit.\n The different versions of MIP  MIP Local: installed at hospitals and providing only local access to the hospital data MIP Federated: the next version that will connect hospitals to a master node and enable distributed queries and statistical analysis.  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]