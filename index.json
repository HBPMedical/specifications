[
{
	"uri": "https://hbpmedical.github.io/specifications/data-flow/",
	"title": "Data flow",
	"tags": [],
	"description": "Flow of data in the platform",
	"content": "One of the main purpose of the Medical Informatics Platform is to collect and process clinical and research data and put into the hand of its users (clinicians, researchers in neuroscience) ready-to-use data containing the variables and features made available for research.\nThe machine learning algorithms and statistical analysis methods provided by the platform are using this feature data when a user is exploring data or performing an experiment.\n Flow in MIP Local  MIP Local is installed in participating hospitals and used to collect clinical data provided by the hospital. Only patients with consent are selected, and no data ever leaves the hospital premises.\n\n Flow in MIP Federation  MIP Federated brings together several hospitals and clinical data centers to build a Federation.\nData are collected at each hospital (clinical data) or clinical data centers (research data), then a user of MIP can query and do machine learning or statistical analyses but with the strong imperative that only data aggregates are exchanged between the hospital or data centers and the central node hosting the user-facing web site.\n\n "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-flow/mip-local/",
	"title": "Flow in MIP Local",
	"tags": [],
	"description": "",
	"content": "MIP Local is installed in participating hospitals and used to collect clinical data provided by the hospital. Only patients with consent are selected, and no data ever leaves the hospital premises.\n\nFlow of data in MIP Local installed at a hospital graph BT subgraph Clinical data or research cohorts in_ehr(EHR database) in_pacs(PACS) end subgraph Data Capture subgraph Anonymiser dc_anon_ehrDepersonalisation of EHR data] dc_anon_mriDepersonalisation of MRI scans] end end subgraph Data Factory subgraph Workflow engine airflow_mri_preprocessingMRI pre-processing] -- airflow_feature_extractionFeature Extraction] airflow_ehr_versionVersioning] -- airflow_ehr_harmoniseHarmonisation] end df_i2b2(I2B2 database) airflow_feature_extraction -- df_i2b2 airflow_ehr_harmonise -- df_i2b2 end subgraph Algorithm Factory hd_features(Features database) --- af_worker(Algorithms) end in_ehr --|Patients with consent| dc_anon_ehr in_pacs --|Patients with consent| dc_anon_mri dc_anon_ehr -- airflow_ehr_version dc_anon_mri -- airflow_mri_preprocessing df_i2b2 -- hd_features "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-flow/mip-federated/",
	"title": "Flow in MIP Federation",
	"tags": [],
	"description": "",
	"content": "MIP Federated brings together several hospitals and clinical data centers to build a Federation.\nData are collected at each hospital (clinical data) or clinical data centers (research data), then a user of MIP can query and do machine learning or statistical analyses but with the strong imperative that only data aggregates are exchanged between the hospital or data centers and the central node hosting the user-facing web site.\n\nFlow of data in MIP Federation graph BT subgraph Hospital #1 databases in_data(EHR database, PACS) end subgraph MIP Local at Hospital #1 subgraph Data Capture dc_anonDepersonalisation] end subgraph Data Factory df_preprocessingPre-processing, feature extraction] end subgraph Algorithm Factory and Local database hd_af(Features database and algorithms) end end in_data --|Patients with consent| dc_anon dc_anon -- df_preprocessing df_preprocessing --hd_af subgraph Hospital #2 databases in2_data(EHR database, PACS) end subgraph MIP Local at Hospital #2 subgraph Data Capture dc2_anonDepersonalisation] end subgraph Data Factory df2_preprocessingPre-processing, feature extraction] end subgraph Algorithm Factory and Local database hd2_af(Features database and algorithms) end end in2_data --|Patients with consent| dc2_anon dc2_anon -- df2_preprocessing df2_preprocessing -- hd2_af subgraph Central node inc_data(Research cohorts) end subgraph MIP Local on central node subgraph Data Capture dcc_anonDepersonalisation] end subgraph Data Factory dfc_preprocessingPre-processing, feature extraction] end subgraph Algorithm Factory and Local database hdc_af(Features database and algorithms) end end inc_data --|Subjects with consent| dcc_anon dcc_anon -- dfc_preprocessing dfc_preprocessing -- hdc_af subgraph Federation layer web(Web analysis) dist_soft(Distributed database and algorithms) web --- dist_soft end hd_af --|Aggregates|dist_soft hd2_af --|Aggregates|dist_soft hdc_af --|Aggregates|dist_soft user((User)) --- web "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/",
	"title": "Data Capture",
	"tags": [],
	"description": "Goals of Data Capture",
	"content": "The main role of the Data Capture is to take data coming several clinical systems in a hospital, collect that data and remove any identifying information from them, then export that data as a collection of CSV files and DICOM/Nifti files to be processed by the Data Factory.\ngraph LR data_in(Disparate clinical data) data_out(Export to data exchange directory) processing Collection + curation + de-identification] data_in -- processing processing -- data_out  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/input/",
	"title": "Data Capture input",
	"tags": [],
	"description": "",
	"content": " Data Capture collects available data from hospital databases or research cohorts.\nIn addition, clinical data are de-personalised to protect patient privacy.\nIt should also track consent and trigger the appropriate mechanisms to remove data from the platform in case the patient has removed her consent.\nIdeal forms of data repository for MIP The MIP has been designed to support a very wide range of methods to gather data, however some datasets are easier to include in the MIP if they meet some criteria.\nDe-personalised data It can be seen as safer and more secure for a hospital to take care of the de-personalisation of its data, especially as this procedure may expose the hospital to ethical and legal questions.\nWe encourage hospitals to provide de-personalised data to MIP, and we can also help to put in place a technical solution, FedEHR\u0026copy; Anonymizer\u0026copy;, provided by our partner Gnubila.\nI2B2 compatible databases MIP uses internally in its Data Factory several databases based on the I2B2 schema. If your data has been prepared for data sharing and follows the I2B2 standard, this effort will fasten the inclusion of your data in the platform.\nPACS system MIP can connect to and query any PACS system compatible with the DICOM standard.\nTo better protect your clinical PACS, we recommend that you put in place a research PACS that will contain only the selection of medical images available for research.\nOther ways of integrating data in the MIP Provided some development or configuration on our side, we can support the following methods of integrating data into MIP.\nFiles extract We can ingest data extract provided as a set of files.\nFor MRI data, files can be in the DICOM or NIFTI formats. We support in particular the BIDS organisation of data\nFor EHR data, files can be in CVS, TSV, JSON or XML formats.\nWeb services We can customise the data capture to use online research databases with a Web Service API.\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-capture/output/",
	"title": "Data Capture output",
	"tags": [],
	"description": "",
	"content": "The Data Capture should export its data into a folder made available to the Data Factory\nThere are several options possible, to adapt to the local requirements:\nDepersonalised DICOM + EHR data export  ├── DICOM │ └── 2016 -- yearly folder, date represents the date of export │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.dcm -- set of DICOM files │ └── dicom_name_generated_02.dcm -- set of DICOM files │ └── dicom_name_generated_03.dcm -- set of DICOM files └── EHR └── 2016 -- yearly folder, date represents the date of export └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  Depersonalised Nifti + EHR data export  ├── NIFTI │ └── 2016 -- yearly folder, date represents the date of export │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.nifti -- Nifti file │ └── dicom_name_generated_01.json -- metadata for the Nifti file │ └── dicom_name_generated_02.nifti -- Nifti file │ └── dicom_name_generated_02.json -- metadata for the Nifti file └── EHR └── 2016 -- yearly folder, date represents the date of export └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  scan_research_id: an ID for research, with no identifier coming the clinical database and representing one visit for one patient. During this visit, there may be more than one scan acquisition session, each session can have several sequences, a sequence can have several repetitions and acquire as many brain scan. One brain scan can be spread into several DICOM files where each file represents a slice of the brain.\nAfter de-identification, we should ensure that patient IDs present in the EHR data match patient IDs present in the DICOM headers.\n "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/",
	"title": "Data Factory",
	"tags": [],
	"description": "Goals of Data Factory",
	"content": " Goals The main role of the Data Factory is to take data coming from Data capture , process it offline (without any user interaction) then store the results into a database of the Hospital Database Bundle.\ngraph LR data_in(Anonymised data) data_out(Data store) processing Pre-processing + ETL + automated curation] data_in -- processing processing -- data_out  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/input/",
	"title": "Data Factory input",
	"tags": [],
	"description": "",
	"content": "The input of Data Factory is the output of Data Capture.\nFrom Data Capture output specifications Depersonalised DICOM + EHR data export  ├── DICOM │ └── 2016 -- yearly folder, date represents the date of export │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.dcm -- set of DICOM files │ └── dicom_name_generated_02.dcm -- set of DICOM files │ └── dicom_name_generated_03.dcm -- set of DICOM files └── EHR └── 2016 -- yearly folder, date represents the date of export └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data  Depersonalised Nifti + EHR data export  ├── NIFTI │ └── 2016 -- yearly folder, date represents the date of export │ └── 20161029 -- daily folder, date represents the date of export │ └── scan_research_id -- see description below │ └── dicom_name_generated_01.nifti -- Nifti file │ └── dicom_name_generated_01.json -- metadata for the Nifti file │ └── dicom_name_generated_02.nifti -- Nifti file │ └── dicom_name_generated_02.json -- metadata for the Nifti file └── EHR └── 2016 -- yearly folder, date represents the date of export └── 20161029 -- daily folder, date represents the date of export ├── table1.csv -- pre-defined name for 1st table containing EHR data, depends on hospital data └── table2.csv -- pre-defined name for 2nd table containing EHR data, depends on hospital data └── ... -- more (or less) tables as needed, depends on hospital data    "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/pipelines/",
	"title": "Processing pipelines",
	"tags": [],
	"description": "",
	"content": " The processing pipelines provided out-of-the-box by the Data Factory enable an automated processing of data made available to MIP Local or MIP Federated.\nOverview of all pipelines graph LR data_in(Anonymised data from Data Capture or other sources) data_out(Research-grade data) reorg_pipeline Reorganisation pipeline] ehr_pipeline EHR ingestion pipeline] metadata_pipeline Metadata ingestion pipeline] preprocessing_pipeline MRI pre-processing and feature extraction pipeline] normalisation_pipeline Normalisation and data export pipeline] data_in -- reorg_pipeline reorg_pipeline -- ehr_pipeline reorg_pipeline -- metadata_pipeline reorg_pipeline -- preprocessing_pipeline ehr_pipeline -- normalisation_pipeline metadata_pipeline -- normalisation_pipeline preprocessing_pipeline -- normalisation_pipeline normalisation_pipeline -- data_out  Reorganisation pipeline This pipeline takes data organised on the disk in its original format and reorganise it into something that fits the layout expected by the following pipelines (EHR, pre-processing, metadata).\ngraph LR data_in(Anonymised data from Data Capture or other sources) data_out(Reorganised data) processing Reorganisation of MRI scans and EHR data] data_in -- processing processing -- data_out  EHR pipeline This pipeline captures as many variables as possible from the patient records and stores the data into a database compliant with I2B2 schema (\u0026lsquo;I2B2 capture\u0026rsquo; database)\ngraph LR data_in(CSV files or other files containing EHR data) data_out(I2B2 capture database) processing ETL with light mapping of EHR data to I2B2 schema] data_in -- processing processing -- data_out  Metadata pipeline This pipeline collects the information associated with MRI scans and present either in DICOM headers or in associated metadata files and store in into the (\u0026lsquo;I2B2 capture\u0026rsquo; database)\ngraph LR data_in(Metadata extracted from MRI scans) data_out(I2B2 capture database) processing ETL with light mapping of metadata to I2B2 schema] data_in -- processing processing -- data_out  MRI pre-processing and feature extraction pipeline This pipeline takes MRI data organised following the directory structure /PatientID/StudyID/SeriesProtocol/SeriesID/ and applies a series of processing steps on it, including:\n conversion from DICOM to Nifti Neuromorphometric pipeline Quality control  For each step, data provenance is tracked and stored in a \u0026lsquo;Data Catalog\u0026rsquo; database.\ngraph LR data_in(MRI scans) data_out(Features stored into 'I2B2 capture' database) processing Neuromorphometric pipeline + quality control + provenance] data_in -- processing processing -- data_out  Normalisation and data export pipeline graph LR data_in('I2B2 capture' database) data_normalised('I2B2 MIP CDE' database) data_out(Features table containing research-grade data) processing Neuromorphometric pipeline + storage of features and provenance] export Export of MIP CDE variables and specific variables to a Features table] data_in -- processing processing -- data_normalised data_normalised -- export export -- data_out  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/data_capture_i2b2/",
	"title": "Data Capture",
	"tags": [],
	"description": "Capture available data and store in into an I2B2 database",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/cde/",
	"title": "Common Data Elements",
	"tags": [],
	"description": "Definition of variables for MIP",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/provenance/",
	"title": "Provenance",
	"tags": [],
	"description": "Tracking provenance information in the Data catalog",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/data_normalisation_i2b2/",
	"title": "Data Normalisation",
	"tags": [],
	"description": "Normalise captured data and store it into an I2B2 database",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/data-factory/output/",
	"title": "Data Factory output",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/algorithm-factory/",
	"title": "Algorithm Factory",
	"tags": [],
	"description": "",
	"content": " Chapter X Some Chapter title Lorem ipsum\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/hospital-database/",
	"title": "Hospital Database",
	"tags": [],
	"description": "",
	"content": " Chapter X Some Chapter title Lorem ipsum\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/credits/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": " Github contributors .ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start } .ghContributors  div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors  div label{ padding-left: 4px ; } .ghContributors  div span{ font-size: x-small; padding-left: 4px ; }  Tooling  Netlify - Continuous deployment and hosting of this documentation Hugo   hugo-theme-docdock    "
},
{
	"uri": "https://hbpmedical.github.io/specifications/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": " Medical Informatics Platform\n"
},
{
	"uri": "https://hbpmedical.github.io/specifications/",
	"title": "MIP specs",
	"tags": [],
	"description": "",
	"content": " Specification for the Medical Informatics Platform  Data flow  Flow of data in the platform\n Data Capture  Goals of Data Capture\n Data Factory  Goals of Data Factory\n Algorithm Factory  Chapter X Some Chapter title Lorem ipsum\n Hospital Database  Chapter X Some Chapter title Lorem ipsum\n The different versions of MIP  MIP Local: installed at hospitals and providing only local access to the hospital data MIP Federated  "
},
{
	"uri": "https://hbpmedical.github.io/specifications/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hbpmedical.github.io/specifications/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]